{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T04:23:06.672215Z",
     "start_time": "2019-02-28T04:23:03.418235Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics import log_loss,confusion_matrix,classification_report,roc_curve,auc, f1_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy import sparse\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "from pymongo import MongoClient, InsertOne, DeleteOne, ReplaceOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T04:23:09.652833Z",
     "start_time": "2019-02-28T04:23:09.609024Z"
    }
   },
   "outputs": [],
   "source": [
    "incel_df = pd.read_csv('new_IncelTears_posts.csv')\n",
    "slate_df = pd.read_csv('new_slatestarcodex_posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T04:23:34.873195Z",
     "start_time": "2019-02-28T04:23:34.438179Z"
    }
   },
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client[\"reddit\"]\n",
    "titles_collection = db.get_collection('titles')\n",
    "overnight_reddit_collection = db.get_collection('overnight_reddit')\n",
    "reddit_overnight_collection = db.get_collection('reddit_overnight') \n",
    "#list(titles_collection.find({'subreddit':'IncelTears', 'over_18':False}).limit(2))\n",
    "\n",
    "with open('fit_undersampled_vect.pickle', 'rb') as handle:\n",
    "     vect_word = pickle.load(handle)\n",
    "        \n",
    "### IMPORTS PICKLED LR MODEL\n",
    "with open('lr_undersampled_model.pickle', 'rb') as handle:\n",
    "     lr = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T04:23:38.365712Z",
     "start_time": "2019-02-28T04:23:38.361345Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_top_X_submittors_to_subreddit(subreddit, X): \n",
    "    subreddit_list = list(titles_collection.find({'subreddit':subreddit}))\n",
    "    subreddit_df = pd.DataFrame(subreddit_list)\n",
    "    subreddit_df = subreddit_df[subreddit_df['author'] != '[deleted]']\n",
    "    top_X_subreddit_submittors = list(subreddit_df.groupby('author').count().sort_values(by=['_id'], ascending=False)[:X].index.values)\n",
    "    return top_X_subreddit_submittors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T04:23:40.721094Z",
     "start_time": "2019-02-28T04:23:40.714528Z"
    }
   },
   "outputs": [],
   "source": [
    "def score_user(subreddit, user):\n",
    "    \"\"\" \n",
    "    This function takes in a subreddit title and user, prints out their toxicity score\n",
    "    and returns a lot of the process for further analysis.\n",
    "    \n",
    "    Right now what it returns are:\n",
    "        toxic_percent = the users' score\n",
    "        toxic_sample, safe sample = 10 sample text to eyeball the usefulness of the model\n",
    "        \n",
    "        These below should eventually be removed.\n",
    "        user_probs = This is currently just appended into the function, eventually it should be pulled out.\n",
    "        Right now what it returns is a predict_proba score instead of a 0,1 for the toxicity.\n",
    "        user_submissions = This is all of the input data which helped me map the worst predict probas\n",
    "        back to their titles to see what the worst predict proba's are. This definitely should also be separated \n",
    "        eventually.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    #top_author = top_3_slate_submittors[0]\n",
    "    user_submissions = list(titles_collection.find({'subreddit':subreddit, 'author':user}))\n",
    "    user_text = np.array([i['title'] for i in user_submissions])\n",
    "    user_vect = vect_word.transform(user_text)\n",
    "    user_preds = lr.predict(user_vect)\n",
    "    user_probs = lr.predict_proba(user_vect) \n",
    "    \n",
    "    toxic_percent = user_preds.sum()/user_preds.shape[0]\n",
    "    print(f'Percentage of {subreddit} user {user} titles predicted as toxic is {round(toxic_percent,2)*100}%')\n",
    "    \n",
    "    toxic_sample = user_text[np.isin(user_preds, 1)][:10] \n",
    "    safe_sample = user_text[np.isin(user_preds, 0)][:10] \n",
    "\n",
    "    return toxic_percent, toxic_sample, safe_sample, user_probs, user_submissions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T05:11:25.794464Z",
     "start_time": "2019-02-28T05:11:22.312682Z"
    }
   },
   "outputs": [],
   "source": [
    "slate_star_top_3 = get_top_X_submittors_to_subreddit('slatestarcodex', 3)\n",
    "incel_tears_top_3 = get_top_X_submittors_to_subreddit('IncelTears', 3)\n",
    "donald_top_3 = get_top_X_submittors_to_subreddit('The_Donald',3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T04:23:51.474626Z",
     "start_time": "2019-02-28T04:23:51.052903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of slatestarcodex user werttrew titles predicted as toxic is 7.000000000000001%\n",
      "Percentage of slatestarcodex user gwern titles predicted as toxic is 10.0%\n",
      "Percentage of slatestarcodex user dwaxe titles predicted as toxic is 5.0%\n",
      "Percentage of IncelTears user RidingChad titles predicted as toxic is 46.0%\n",
      "Percentage of IncelTears user BrazilianSigma titles predicted as toxic is 47.0%\n",
      "Percentage of IncelTears user caspertruth666 titles predicted as toxic is 41.0%\n"
     ]
    }
   ],
   "source": [
    "slate_top_3_scores_and_samples = [score_user('slatestarcodex',i) for i in slate_star_top_3]\n",
    "incel_top_3_scores_and_samples = [score_user('IncelTears',i) for i in incel_tears_top_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T05:20:18.596943Z",
     "start_time": "2019-02-28T05:20:18.590710Z"
    }
   },
   "outputs": [],
   "source": [
    "def users_most_toxic_submissions(subreddit, user, n_submissions):\n",
    "    toxic_percent, toxic_sample, safe_sample, user_probs, user_submissions = score_user(subreddit, user)\n",
    "    idx_value_probs = [(idx, value) for idx, value in enumerate(user_probs)]\n",
    "    check_5_highest_toxicity = sorted(idx_value_probs, reverse=True, key=lambda x: x[1][1])[:n_submissions]\n",
    "    \n",
    "    #add time stamps into the return for future time based toxicity analysis\n",
    "    n_highest_proba_time_and_text = [(i[1][1], \n",
    "        datetime.utcfromtimestamp(user_submissions[i[0]]['created_utc']).strftime('%Y-%m-%d %H:%M:%S'), \n",
    "        user_submissions[i[0]]['title']) for i in check_5_highest_toxicity]\n",
    "    n_highest_probab_and_text = list(zip(n_highest_proba_time_and_text[0], n_highest_proba_time_and_text[2]))\n",
    "    n_highest_text = [i[1] for i in n_highest_proba_and_text]\n",
    "    return n_highest_proba_and_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T05:20:20.158166Z",
     "start_time": "2019-02-28T05:20:20.079837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of slatestarcodex user werttrew titles predicted as toxic is 7.000000000000001%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.9088324983834649, 'We are all confident idiots'),\n",
       " (0.8565731683539299, 'INTERLUDE ו: THERE’S A HOLE IN MY BUCKET (Unsong)'),\n",
       " (0.8276116381940869,\n",
       "  '\"A new definition of the nerd: a person who knows his own mind well enough to mistrust it\"'),\n",
       " (0.7964137435474457,\n",
       "  'Why Are Babies So Dumb If Humans Are So Smart? (The New Yorker)'),\n",
       " (0.7879200032704085,\n",
       "  '“The Suck Fairy”: when you reread a beloved book and it loses its charm for you (2010)')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_most_toxic_submissions('slatestarcodex', slate_star_top_3[0], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T05:17:32.809265Z",
     "start_time": "2019-02-28T05:17:32.806014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-11-27 22:51:41\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "#ts = int(\"1284101485\")\n",
    "\n",
    "# if you encounter a \"year is out of range\" error the timestamp\n",
    "# may be in milliseconds, try `ts /= 1000` in that case\n",
    "print(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T05:12:45.892762Z",
     "start_time": "2019-02-28T05:12:45.801883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of IncelTears user RidingChad titles predicted as toxic is 46.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.9997579343233027, \"Incel's - Fuck the Jews\"),\n",
       " (0.9944216756594696,\n",
       "  'Incel - yeah, rape sucks but being Incel is so much worse'),\n",
       " (0.9922389196878674, 'Incel hates gay people'),\n",
       " (0.9905268650282755, 'Stupid Normies defend Muslims but not Elliot Rodger'),\n",
       " (0.9846286227093788,\n",
       "  'TIL-You can tell if a woman has had sex by the shape of her ass'),\n",
       " (0.9831589416253615, 'Sex is evil'),\n",
       " (0.9827044018804086, \"But they don't hate women guys\"),\n",
       " (0.9811006676799212,\n",
       "  \"Stupid women don't even know what's attractive to them\"),\n",
       " (0.9775923574595929, 'Holy Hell Incels • r/justneckbeardthings'),\n",
       " (0.9775557872756588,\n",
       "  \"Fuck old people. It's Grandma and Grandpa's fault I'm an Incel\")]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_most_toxic_submissions('IncelTears', incel_tears_top_3[0], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T05:12:47.666453Z",
     "start_time": "2019-02-28T05:12:47.596662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of The_Donald user VoteForTrump2016 titles predicted as toxic is 11.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.9302587659678289,\n",
       "  \"Donald Trump's supporters are not racist – they are sick of being let down\"),\n",
       " (0.8270316013686174,\n",
       "  \"Calling Donald Trump a 'jerk' doesn't seem to be helping Jeb Bush\"),\n",
       " (0.8140475862380802,\n",
       "  'Trump: Fellow Republicans are ‘jealous as hell’ of Putin’s praise'),\n",
       " (0.8098818320057566, \"Trump: Bush 'dumb as a rock'\"),\n",
       " (0.8092709175587026, \"Jeb Bush: 'Donald Trump is a jerk'\"),\n",
       " (0.7870263852344647,\n",
       "  'Herman Cain: Unlike me, Donald Trump has money to sue people ‘who will lie against him’'),\n",
       " (0.7717921340857941, \"Donald Trump: Hillary Clinton 'lies like crazy'\"),\n",
       " (0.754083677058401, \"Jindal: 'A nice guy' who was 'a little nasty'\"),\n",
       " (0.7272623325784766,\n",
       "  \"Who's the dumb one? Obama reacts to Trump climate criticism\"),\n",
       " (0.7148984569353652,\n",
       "  'Donald Trump: \"These Politicians Are All Talk,\" \"It Is All Bullshit\"')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_most_toxic_submissions('The_Donald', donald_top_3[0], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxicity over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T05:24:33.549666Z",
     "start_time": "2019-02-28T05:24:33.474733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of slatestarcodex user werttrew titles predicted as toxic is 7.000000000000001%\n"
     ]
    }
   ],
   "source": [
    "toxic_percent, toxic_sample, safe_sample, user_probs, user_submissions = score_user('slatestarcodex', slate_star_top_3[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decided to do this tomorrow - graph toxicity over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T05:25:56.923017Z",
     "start_time": "2019-02-28T05:25:56.918826Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1442667652"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time,title = user_submissions[4]['created_utc'], user_submissions[4]['title']\n",
    "\n",
    "datetime.utcfromtimestamp(user_submissions[i[0]]['created_utc']).strftime('%Y-%m-%d %H:%M:%S'), \n",
    "\n",
    "user_submissions[4]['created_utc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comments_by_time(subreddit, user):\n",
    "    toxic_percent, toxic_sample, safe_sample, user_probs, user_submissions = score_user(subreddit, user)\n",
    "    \n",
    "    \n",
    "    \n",
    "    idx_value_probs = [(idx, value) for idx, value in enumerate(user_probs)]\n",
    "    check_5_highest_toxicity = sorted(idx_value_probs, reverse=True, key=lambda x: x[1][1])[:10] #remove this\n",
    "    \n",
    "    #add time stamps into the return for future time based toxicity analysis\n",
    "    n_highest_proba_time_and_text = [(i[1][1], \n",
    "        datetime.utcfromtimestamp(user_submissions[i[0]]['created_utc']).strftime('%Y-%m-%d %H:%M:%S'), \n",
    "        user_submissions[i[0]]['title']) for i in check_5_highest_toxicity]\n",
    "    n_highest_probab_and_text = list(zip(n_highest_proba_time_and_text[0], n_highest_proba_time_and_text[2]))\n",
    "    n_highest_text = [i[1] for i in n_highest_proba_and_text]\n",
    "    return n_highest_proba_and_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
