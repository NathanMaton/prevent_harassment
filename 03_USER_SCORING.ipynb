{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:43:45.248081Z",
     "start_time": "2019-02-28T00:43:44.150368Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics import log_loss,confusion_matrix,classification_report,roc_curve,auc, f1_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy import sparse\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "from pymongo import MongoClient, InsertOne, DeleteOne, ReplaceOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:43:45.738639Z",
     "start_time": "2019-02-28T00:43:45.714158Z"
    }
   },
   "outputs": [],
   "source": [
    "incel_df = pd.read_csv('new_IncelTears_posts.csv')\n",
    "slate_df = pd.read_csv('new_slatestarcodex_posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:43:46.517334Z",
     "start_time": "2019-02-28T00:43:46.507437Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['titles',\n",
       " 'test_insert',\n",
       " 'mycollection',\n",
       " 'overnight_reddit',\n",
       " 'reddit_overnight']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = MongoClient()\n",
    "db = client[\"reddit\"]\n",
    "db.collection_names()\n",
    "#db.create_collection(\"test_insert\")\n",
    "#test_collection = db.get_collection('test_insert')\n",
    "\n",
    "#db.create_collection(\"titles\")\n",
    "#titles_collection = db.get_collection('titles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:43:47.565792Z",
     "start_time": "2019-02-28T00:43:47.561657Z"
    }
   },
   "outputs": [],
   "source": [
    "titles_collection = db.get_collection('titles')\n",
    "overnight_reddit_collection = db.get_collection('overnight_reddit')\n",
    "reddit_overnight_collection = db.get_collection('reddit_overnight') \n",
    "#list(titles_collection.find({'subreddit':'IncelTears', 'over_18':False}).limit(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:45:10.575511Z",
     "start_time": "2019-02-28T00:45:10.189678Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('fit_undersampled_vect.pickle', 'rb') as handle:\n",
    "     vect_word = pickle.load(handle)\n",
    "        \n",
    "### IMPORTS PICKLED LR MODEL\n",
    "with open('lr_undersampled_model.pickle', 'rb') as handle:\n",
    "     lr = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:45:11.234212Z",
     "start_time": "2019-02-28T00:45:11.229646Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_top_X_submittors_to_subreddit(subreddit, X): \n",
    "    subreddit_list = list(titles_collection.find({'subreddit':subreddit}))\n",
    "    subreddit_df = pd.DataFrame(subreddit_list)\n",
    "    subreddit_df = subreddit_df[subreddit_df['author'] != '[deleted]']\n",
    "    top_X_subreddit_submittors = list(subreddit_df.groupby('author').count().sort_values(by=['_id'], ascending=False)[:X].index.values)\n",
    "    return top_X_subreddit_submittors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:45:11.577510Z",
     "start_time": "2019-02-28T00:45:11.571205Z"
    }
   },
   "outputs": [],
   "source": [
    "def score_user(subreddit, user):\n",
    "    \"\"\" \n",
    "    This function takes in a subreddit title and user, prints out their toxicity score\n",
    "    and returns a lot of the process for further analysis.\n",
    "    \n",
    "    Right now what it returns are:\n",
    "        toxic_percent = the users' score\n",
    "        toxic_sample, safe sample = 10 sample text to eyeball the usefulness of the model\n",
    "        \n",
    "        These below should eventually be removed.\n",
    "        user_probs = This is currently just appended into the function, eventually it should be pulled out.\n",
    "        Right now what it returns is a predict_proba score instead of a 0,1 for the toxicity.\n",
    "        user_submissions = This is all of the input data which helped me map the worst predict probas\n",
    "        back to their titles to see what the worst predict proba's are. This definitely should also be separated \n",
    "        eventually.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    #top_author = top_3_slate_submittors[0]\n",
    "    user_submissions = list(titles_collection.find({'subreddit':subreddit, 'author':user}))\n",
    "    user_text = np.array([i['title'] for i in user_submissions])\n",
    "    user_vect = vect_word.transform(user_text)\n",
    "    user_preds = lr.predict(user_vect)\n",
    "    user_probs = lr.predict_proba(user_vect) \n",
    "    \n",
    "    toxic_percent = user_preds.sum()/user_preds.shape[0]\n",
    "    print(f'Percentage of {subreddit} user {user} titles predicted as toxic is {round(toxic_percent,2)*100}%')\n",
    "    \n",
    "    toxic_sample = user_text[np.isin(user_preds, 1)][:10] \n",
    "    safe_sample = user_text[np.isin(user_preds, 0)][:10] \n",
    "\n",
    "    return toxic_percent, toxic_sample, safe_sample, user_probs, user_submissions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:45:14.434996Z",
     "start_time": "2019-02-28T00:45:12.054683Z"
    }
   },
   "outputs": [],
   "source": [
    "slate_star_top_3 = get_top_X_submittors_to_subreddit('slatestarcodex', 3)\n",
    "incel_tears_top_3 = get_top_X_submittors_to_subreddit('IncelTears', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:45:14.844912Z",
     "start_time": "2019-02-28T00:45:14.437276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of slatestarcodex user werttrew titles predicted as toxic is 7.000000000000001%\n",
      "Percentage of slatestarcodex user gwern titles predicted as toxic is 10.0%\n",
      "Percentage of slatestarcodex user dwaxe titles predicted as toxic is 5.0%\n",
      "Percentage of IncelTears user RidingChad titles predicted as toxic is 46.0%\n",
      "Percentage of IncelTears user BrazilianSigma titles predicted as toxic is 47.0%\n",
      "Percentage of IncelTears user caspertruth666 titles predicted as toxic is 41.0%\n"
     ]
    }
   ],
   "source": [
    "slate_top_3_scores_and_samples = [score_user('slatestarcodex',i) for i in slate_star_top_3]\n",
    "incel_top_3_scores_and_samples = [score_user('IncelTears',i) for i in incel_tears_top_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:45:14.853855Z",
     "start_time": "2019-02-28T00:45:14.847099Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We are all confident idiots',\n",
       " 'INTERLUDE ו: THERE’S A HOLE IN MY BUCKET (Unsong)',\n",
       " '\"A new definition of the nerd: a person who knows his own mind well enough to mistrust it\"',\n",
       " 'Why Are Babies So Dumb If Humans Are So Smart? (The New Yorker)',\n",
       " '“The Suck Fairy”: when you reread a beloved book and it loses its charm for you (2010)']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top users' predict proba's sorted by the ones most likely to be toxic listing top 5. - SLATE\n",
    "user_predict_proba = [(idx, value) for idx, value in enumerate(slate_top_3_scores_and_samples[0][3])]\n",
    "check_5_highest_toxicity = sorted(user_predict_proba, reverse=True, key=lambda x: x[1][1])[:5]\n",
    "five_highest_text = [slate_top_3_scores_and_samples[0][4][i[0]]['title'] for i in check_5_highest_toxicity]\n",
    "five_highest_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:46:54.834869Z",
     "start_time": "2019-02-28T00:46:54.830799Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Incel tells rape victim to stop using it as a crutch',\n",
       "       'Women dress like sluts (how dare they) and men made them',\n",
       "       'Woman compliments Incel in front of her husband. How dare she?',\n",
       "       \"Pedocel thinks mother's are jealous not protective of their daughters\",\n",
       "       'Paging Neve and Max - Incels just love that catfishing',\n",
       "       'Wishing suffering and death penalty on Mom',\n",
       "       'Incel asked single mom to kill her child for him',\n",
       "       \"Incel rants about his sister's sex life, other Incels request pictures\",\n",
       "       'No, Filipino women are not going to put up with your personality either',\n",
       "       'Peepingtomcel wants others to die in a fire'], dtype='<U169')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incel_top_3_scores_and_samples[0][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:54:59.813416Z",
     "start_time": "2019-02-28T00:54:59.809056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.37115451, -0.71185306,  0.19783052, ...,  0.90005951,\n",
       "        -0.04174385, -0.03246744]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T00:52:27.664840Z",
     "start_time": "2019-02-28T00:52:27.652224Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(138, array([2.42065677e-04, 9.99757934e-01])),\n",
       " (113, array([0.00557832, 0.99442168])),\n",
       " (174, array([0.00776108, 0.99223892])),\n",
       " (288, array([0.00947313, 0.99052687])),\n",
       " (202, array([0.01537138, 0.98462862])),\n",
       " (771, array([0.01684106, 0.98315894])),\n",
       " (551, array([0.0172956, 0.9827044])),\n",
       " (601, array([0.01889933, 0.98110067])),\n",
       " (230, array([0.02240764, 0.97759236])),\n",
       " (69, array([0.02244421, 0.97755579])),\n",
       " (613, array([0.02268242, 0.97731758])),\n",
       " (513, array([0.02583673, 0.97416327])),\n",
       " (32, array([0.02737394, 0.97262606])),\n",
       " (239, array([0.02925115, 0.97074885])),\n",
       " (735, array([0.03362948, 0.96637052])),\n",
       " (632, array([0.0414727, 0.9585273])),\n",
       " (559, array([0.04338125, 0.95661875])),\n",
       " (704, array([0.04880665, 0.95119335])),\n",
       " (9, array([0.05187268, 0.94812732])),\n",
       " (683, array([0.05321626, 0.94678374]))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top users' predict proba's sorted by the ones most likely to be toxic listing top 5. - INCEL\n",
    "user_predict_proba = [(idx, value) for idx, value in enumerate(incel_top_3_scores_and_samples[0][3])]\n",
    "check_5_highest_toxicity = sorted(user_predict_proba, reverse=True, key=lambda x: x[1][1])[:20]\n",
    "five_highest_text = [slate_top_3_scores_and_samples[0][4][i[0]]['title'] for i in check_5_highest_toxicity]\n",
    "# five_highest_text\n",
    "check_5_highest_toxicity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
