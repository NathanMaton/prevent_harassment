{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d894877c-753f-4947-a6a0-8c100b8af6b2",
    "_uuid": "3a208d285d49bbe7c35827de8416e3b7c5c061ae"
   },
   "source": [
    "## Toxic comment classification\n",
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T21:31:43.120201Z",
     "start_time": "2019-03-01T21:31:40.037817Z"
    },
    "_cell_guid": "fd0d94af-8dcd-4258-92fc-d1c304215a9a",
    "_uuid": "d2539467b6d1fa164da8c43825cd30a124eb9c47"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics import log_loss,confusion_matrix,classification_report,roc_curve,auc, f1_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b4043cdf-f986-42bc-ad09-ea124e152507",
    "_uuid": "6b388128bf18f28c29d66377e9deb5f1ea8067f1"
   },
   "source": [
    "## Read data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T21:31:44.570692Z",
     "start_time": "2019-03-01T21:31:43.122573Z"
    },
    "_cell_guid": "7ce644b7-5332-40d7-a827-15f6897be5e8",
    "_uuid": "d1134807fc7b6c604f7fbdd42e0f27e69a834337"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and columns in the train data set: (159571, 8)\n"
     ]
    }
   ],
   "source": [
    "toxic = pd.read_csv('toxicity_data/train.csv') #there's also a test dataset but it doesn't have labels b/c kaggle.\n",
    "print('Number of rows and columns in the train data set:',toxic.shape)\n",
    "\n",
    "#unlabeled data\n",
    "incel_df = pd.read_csv('new_IncelTears_posts.csv')\n",
    "slate_df = pd.read_csv('new_slatestarcodex_posts.csv')\n",
    "\n",
    "#turn multi-class into single class classifier\n",
    "target_col = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']\n",
    "y = toxic[target_col]\n",
    "y['sum'] = y.sum(axis=1).astype(bool).astype(int)\n",
    "\n",
    "#splits data, creates holdout dataset\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(toxic, y, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3a73e542-178e-4b3e-ad8c-87e9dc659762",
    "_uuid": "2daff7626ba426dbfa0172b6c3a4351af27cd4e9"
   },
   "source": [
    "## Text preprocessing - TF-IDF up to trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T21:32:21.853048Z",
     "start_time": "2019-03-01T21:31:44.572507Z"
    },
    "_cell_guid": "93502afb-68c7-4cc2-ad03-f71d2b2cbf2a",
    "_uuid": "3d1747c73d3c67c93eb4e7e81de4400276f0580c"
   },
   "outputs": [],
   "source": [
    "vect_word = TfidfVectorizer(max_features=20000, lowercase=True, analyzer='word',\n",
    "                        stop_words= 'english',ngram_range=(1,3),dtype=np.float32)\n",
    "tr_vect = vect_word.fit_transform(X_train['comment_text'])\n",
    "ts_vect = vect_word.transform(X_test['comment_text'])\n",
    "\n",
    "incel_vect = vect_word.transform(incel_df['title'])\n",
    "slate_vect = vect_word.transform(slate_df['title'])\n",
    "\n",
    "#took 50 seconds on 150k samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fab7e57c-fa64-4540-a53e-085f849d42ca",
    "_uuid": "b15c44a583628a0a72036536e8a5fdb67273a4ae"
   },
   "source": [
    "## LR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T21:32:23.711948Z",
     "start_time": "2019-03-01T21:32:21.855491Z"
    },
    "_cell_guid": "7f5b18c2-6775-4493-ae4b-d7a2456dbdd2",
    "_uuid": "39da8aecea6496cadd1aa2133431cceb440b390c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix\n",
      " [[21668  1211]\n",
      " [  428  2225]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96     22879\n",
      "           1       0.65      0.84      0.73      2653\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     25532\n",
      "   macro avg       0.81      0.89      0.85     25532\n",
      "weighted avg       0.95      0.94      0.94     25532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C=2,random_state = 42,class_weight = 'balanced')\n",
    "lr.fit(tr_vect,y_train['sum'])\n",
    "\n",
    "pred =  lr.predict(ts_vect)\n",
    "print('\\nConfusion matrix\\n',confusion_matrix(y_test['sum'],pred))\n",
    "print(classification_report(y_test['sum'],pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T21:32:23.723346Z",
     "start_time": "2019-03-01T21:32:23.713961Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7308260798160617"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test['sum'],pred)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T21:32:23.757031Z",
     "start_time": "2019-03-01T21:32:23.725048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuck                 \t\t thanks              \n",
      "fucking              \t\t best                \n",
      "shit                 \t\t talk                \n",
      "idiot                \t\t interested          \n",
      "stupid               \t\t thank               \n",
      "ass                  \t\t stop vandalizing    \n",
      "bullshit             \t\t consensus           \n",
      "asshole              \t\t article             \n",
      "bitch                \t\t ips                 \n",
      "suck                 \t\t wikiproject         \n",
      "cunt                 \t\t wp                  \n",
      "crap                 \t\t mentioned           \n",
      "moron                \t\t future              \n",
      "dick                 \t\t test                \n",
      "faggot               \t\t agree               \n",
      "sucks                \t\t help                \n",
      "idiots               \t\t official            \n",
      "penis                \t\t request             \n",
      "jerk                 \t\t appreciate          \n",
      "hell                 \t\t dispute             \n",
      "pathetic             \t\t title               \n",
      "bastard              \t\t disagree            \n",
      "fucked               \t\t template            \n",
      "nigger               \t\t early               \n",
      "dumbass              \t\t placed              \n",
      "ignorant             \t\t category            \n",
      "loser                \t\t company             \n",
      "dumb                 \t\t section             \n",
      "liar                 \t\t believe             \n",
      "idiotic              \t\t redirect talk       \n",
      "shut                 \t\t related             \n",
      "wtf                  \t\t considered          \n",
      "cock                 \t\t named               \n",
      "gay                  \t\t 30                  \n",
      "stupidity            \t\t numbers             \n",
      "motherfucker         \t\t copy                \n",
      "piss                 \t\t background          \n",
      "fag                  \t\t west                \n",
      "damn                 \t\t necessary           \n",
      "morons               \t\t light               \n",
      "pig                  \t\t assumed             \n",
      "fuckin               \t\t review              \n",
      "hypocrite            \t\t chances             \n",
      "fool                 \t\t age                 \n",
      "retarded             \t\t infobox             \n",
      "pussy                \t\t season              \n",
      "dickhead             \t\t material            \n",
      "sex                  \t\t apologize           \n",
      "ridiculous           \t\t question            \n",
      "fucker               \t\t semi                \n",
      "bastards             \t\t month               \n",
      "retard               \t\t paragraph           \n",
      "assholes             \t\t central             \n",
      "bitches              \t\t knowledge           \n",
      "ck                   \t\t process             \n",
      "racist               \t\t recently            \n",
      "kill                 \t\t source              \n",
      "bloody               \t\t original            \n",
      "die                  \t\t sources             \n",
      "twat                 \t\t learned             \n",
      "cocksucker           \t\t cheers              \n",
      "hate                 \t\t left                \n",
      "screw                \t\t moved               \n",
      "sick                 \t\t impossible          \n",
      "coward               \t\t mention             \n",
      "arse                 \t\t conflict            \n",
      "arrogant             \t\t map                 \n",
      "ugly                 \t\t smith               \n",
      "nazis                \t\t commons             \n",
      "scum                 \t\t include             \n",
      "porn                 \t\t welcome             \n",
      "jackass              \t\t involved            \n",
      "goddamn              \t\t removed             \n",
      "homo                 \t\t tag                 \n",
      "homosexual           \t\t 119                 \n",
      "fat                  \t\t redirects           \n",
      "dicks                \t\t using               \n",
      "pedophile            \t\t characters          \n",
      "cum                  \t\t applied             \n",
      "ing                  \t\t vote                \n",
      "wanker               \t\t book                \n",
      "dirty                \t\t 2008                \n",
      "balls                \t\t justified           \n",
      "whore                \t\t religion            \n",
      "butt                 \t\t sent                \n",
      "fucks                \t\t raised              \n",
      "shitty               \t\t redirect            \n",
      "douchebag            \t\t published           \n",
      "trash                \t\t standard            \n",
      "moronic              \t\t continued           \n",
      "nerd                 \t\t sure                \n",
      "dipshit              \t\t thinking            \n",
      "losers               \t\t chat                \n",
      "rubbish              \t\t weight              \n",
      "anal                 \t\t connection          \n",
      "disgusting           \t\t fine                \n",
      "mouth                \t\t movement            \n",
      "lick                 \t\t instance            \n",
      "nazi                 \t\t marriage            \n",
      "cking                \t\t neutral             \n"
     ]
    }
   ],
   "source": [
    "model_coefs = lr.coef_[0,:]\n",
    "model_keys = np.argsort(model_coefs)\n",
    "vocab_dict = {v: k for k, v in vect_word.vocabulary_.items()}\n",
    "top_n_words = 100\n",
    "pos_words = model_keys[-top_n_words::][::-1]\n",
    "neg_words = model_keys[:top_n_words]\n",
    "\n",
    "for p, n in zip(pos_words, neg_words):\n",
    "    print(f\"{vocab_dict[p]: <20} \\t\\t {vocab_dict[n]: <20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
