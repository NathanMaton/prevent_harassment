{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T22:59:08.235016Z",
     "start_time": "2019-03-03T22:59:07.078632Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics import log_loss,confusion_matrix,classification_report,roc_curve,auc, f1_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy import sparse\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "from pymongo import MongoClient, InsertOne, DeleteOne, ReplaceOne\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Mongo & Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T22:59:09.156577Z",
     "start_time": "2019-03-03T22:59:08.749605Z"
    }
   },
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client[\"reddit\"]\n",
    "titles_collection = db.get_collection('titles')\n",
    "overnight_reddit_collection = db.get_collection('overnight_reddit')\n",
    "reddit_overnight_collection = db.get_collection('reddit_overnight') \n",
    "#list(titles_collection.find({'subreddit':'IncelTears', 'over_18':False}).limit(2))\n",
    "\n",
    "with open('fit_undersampled_vect.pickle', 'rb') as handle:\n",
    "     vect_word = pickle.load(handle)\n",
    "        \n",
    "### IMPORTS PICKLED LR MODEL\n",
    "with open('lr_undersampled_model.pickle', 'rb') as handle:\n",
    "     lr = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T23:01:17.727294Z",
     "start_time": "2019-03-03T23:01:17.722965Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_top_X_submittors_to_subreddit(subreddit, X): \n",
    "    subreddit_list = list(titles_collection.find({'subreddit':subreddit}))\n",
    "    subreddit_df = pd.DataFrame(subreddit_list)\n",
    "    subreddit_df = subreddit_df[subreddit_df['author'] != '[deleted]']\n",
    "    top_X_subreddit_submittors = list(subreddit_df.groupby('author').count().sort_values(by=['_id'], ascending=False)[:X].index.values)\n",
    "    return top_X_subreddit_submittors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T23:01:17.984365Z",
     "start_time": "2019-03-03T23:01:17.978061Z"
    }
   },
   "outputs": [],
   "source": [
    "def score_user(subreddit, user):\n",
    "    \"\"\" \n",
    "    This function takes in a subreddit title and user, prints out their toxicity score\n",
    "    and returns a lot of the process for further analysis.\n",
    "    \n",
    "    Right now what it returns are:\n",
    "        toxic_percent = the users' score\n",
    "        toxic_sample, safe sample = 10 sample text to eyeball the usefulness of the model\n",
    "        \n",
    "        These below should eventually be removed.\n",
    "        user_probs = This is currently just appended into the function, eventually it should be pulled out.\n",
    "        Right now what it returns is a predict_proba score instead of a 0,1 for the toxicity.\n",
    "        user_submissions = This is all of the input data which helped me map the worst predict probas\n",
    "        back to their titles to see what the worst predict proba's are. This definitely should also be separated \n",
    "        eventually.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    #top_author = top_3_slate_submittors[0]\n",
    "    user_submissions = list(titles_collection.find({'subreddit':subreddit, 'author':user}))\n",
    "    user_text = np.array([i['title'] for i in user_submissions])\n",
    "    user_vect = vect_word.transform(user_text)\n",
    "    user_preds = lr.predict(user_vect)\n",
    "    user_probs = lr.predict_proba(user_vect) \n",
    "    \n",
    "    toxic_percent = user_preds.sum()/user_preds.shape[0]\n",
    "    print(f'Percentage of {subreddit} user {user} titles predicted as toxic is {round(toxic_percent,2)*100}%')\n",
    "    \n",
    "    toxic_sample = user_text[np.isin(user_preds, 1)][:10] \n",
    "    safe_sample = user_text[np.isin(user_preds, 0)][:10] \n",
    "\n",
    "    return toxic_percent, toxic_sample, safe_sample, user_probs, user_submissions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T23:01:22.750330Z",
     "start_time": "2019-03-03T23:01:18.281919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of slatestarcodex user werttrew titles predicted as toxic is 7.000000000000001%\n",
      "Percentage of slatestarcodex user gwern titles predicted as toxic is 10.0%\n",
      "Percentage of slatestarcodex user dwaxe titles predicted as toxic is 5.0%\n",
      "Percentage of IncelTears user RidingChad titles predicted as toxic is 46.0%\n",
      "Percentage of IncelTears user BrazilianSigma titles predicted as toxic is 47.0%\n",
      "Percentage of IncelTears user caspertruth666 titles predicted as toxic is 41.0%\n"
     ]
    }
   ],
   "source": [
    "slate_star_top_3 = get_top_X_submittors_to_subreddit('slatestarcodex', 3)\n",
    "incel_tears_top_3 = get_top_X_submittors_to_subreddit('IncelTears', 3)\n",
    "donald_top_3 = get_top_X_submittors_to_subreddit('The_Donald',3)\n",
    "slate_top_3_scores_and_samples = [score_user('slatestarcodex',i) for i in slate_star_top_3]\n",
    "incel_top_3_scores_and_samples = [score_user('IncelTears',i) for i in incel_tears_top_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Probability of toxicity to start thinking about banning criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T23:01:22.759800Z",
     "start_time": "2019-03-03T23:01:22.752471Z"
    }
   },
   "outputs": [],
   "source": [
    "def users_most_toxic_submissions(subreddit, user, n_submissions):\n",
    "    toxic_percent, toxic_sample, safe_sample, user_probs, user_submissions = score_user(subreddit, user)\n",
    "    idx_value_probs = [(idx, value) for idx, value in enumerate(user_probs)]\n",
    "    check_5_highest_toxicity = sorted(idx_value_probs, reverse=True, key=lambda x: x[1][1])[:n_submissions]\n",
    "    \n",
    "    #add time stamps into the return for future time based toxicity analysis\n",
    "    n_highest_proba_time_and_text = [(i[1][1], \n",
    "        datetime.utcfromtimestamp(user_submissions[i[0]]['created_utc']).strftime('%Y-%m-%d %H:%M:%S'), \n",
    "        user_submissions[i[0]]['title']) for i in check_5_highest_toxicity]\n",
    "    n_highest_proba_and_text = list(zip(n_highest_proba_time_and_text[0], n_highest_proba_time_and_text[2]))\n",
    "    n_highest_text = [i[1] for i in n_highest_proba_and_text]\n",
    "    return n_highest_proba_and_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T23:01:22.880535Z",
     "start_time": "2019-03-03T23:01:22.761485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of slatestarcodex user werttrew titles predicted as toxic is 7.000000000000001%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.9088324983834649, 0.8276116381940869),\n",
       " ('2015-11-27 22:51:41', '2017-12-03 15:48:25'),\n",
       " ('We are all confident idiots',\n",
       "  '\"A new definition of the nerd: a person who knows his own mind well enough to mistrust it\"')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_most_toxic_submissions('slatestarcodex', slate_star_top_3[0], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T23:01:23.003407Z",
     "start_time": "2019-03-03T23:01:22.882813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of IncelTears user RidingChad titles predicted as toxic is 46.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.9997579343233027, 0.9922389196878674),\n",
       " ('2017-06-14 02:39:21', '2017-06-21 02:16:04'),\n",
       " (\"Incel's - Fuck the Jews\", 'Incel hates gay people')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_most_toxic_submissions('IncelTears', incel_tears_top_3[0], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T23:01:23.104343Z",
     "start_time": "2019-03-03T23:01:23.005380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of The_Donald user VoteForTrump2016 titles predicted as toxic is 11.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.9302587659678289, 0.8140475862380802),\n",
       " ('2015-12-29 21:01:57', '2015-12-20 20:22:08'),\n",
       " (\"Donald Trump's supporters are not racist – they are sick of being let down\",\n",
       "  'Trump: Fellow Republicans are ‘jealous as hell’ of Putin’s praise')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_most_toxic_submissions('The_Donald', donald_top_3[0], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxicity over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T23:01:25.779180Z",
     "start_time": "2019-03-03T23:01:25.674108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of slatestarcodex user werttrew titles predicted as toxic is 7.000000000000001%\n"
     ]
    }
   ],
   "source": [
    "toxic_percent, toxic_sample, safe_sample, user_probs, user_submissions = score_user('slatestarcodex', slate_star_top_3[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consider graphing over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code below needs revisiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T23:02:01.813390Z",
     "start_time": "2019-03-03T23:02:01.807085Z"
    }
   },
   "outputs": [],
   "source": [
    "def comments_by_time(subreddit, user):\n",
    "    toxic_percent, toxic_sample, safe_sample, user_probs, user_submissions = score_user(subreddit, user)\n",
    "\n",
    "    idx_value_probs = [(idx, value) for idx, value in enumerate(user_probs)]\n",
    "    check_5_highest_toxicity = sorted(idx_value_probs, reverse=True, key=lambda x: x[1][1])[:10] #remove this\n",
    "    \n",
    "    #add time stamps into the return for future time based toxicity analysis\n",
    "    n_highest_proba_time_and_text = [(i[1][1], \n",
    "        datetime.utcfromtimestamp(user_submissions[i[0]]['created_utc']).strftime('%Y-%m-%d %H:%M:%S'), \n",
    "        user_submissions[i[0]]['title']) for i in check_5_highest_toxicity]\n",
    "    n_highest_probab_and_text = list(zip(n_highest_proba_time_and_text[0], n_highest_proba_time_and_text[2]))\n",
    "    n_highest_text = [i[1] for i in n_highest_proba_and_text]\n",
    "    return n_highest_proba_and_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-03T23:01:27.183025Z",
     "start_time": "2019-03-03T23:01:27.170836Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6f52fa99f979>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_submissions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'created_utc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_submissions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutcfromtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_submissions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'created_utc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y-%m-%d %H:%M:%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0muser_submissions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'created_utc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "# time,title = user_submissions[4]['created_utc'], user_submissions[4]['title']\n",
    "\n",
    "# datetime.utcfromtimestamp(user_submissions[i[0]]['created_utc']).strftime('%Y-%m-%d %H:%M:%S'), \n",
    "\n",
    "# user_submissions[4]['created_utc']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
