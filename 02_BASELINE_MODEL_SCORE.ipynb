{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d894877c-753f-4947-a6a0-8c100b8af6b2",
    "_uuid": "3a208d285d49bbe7c35827de8416e3b7c5c061ae"
   },
   "source": [
    "## Toxic comment classification\n",
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T18:13:18.363153Z",
     "start_time": "2019-03-01T18:13:17.330951Z"
    },
    "_cell_guid": "fd0d94af-8dcd-4258-92fc-d1c304215a9a",
    "_uuid": "d2539467b6d1fa164da8c43825cd30a124eb9c47"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics import log_loss,confusion_matrix,classification_report,roc_curve,auc, f1_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b4043cdf-f986-42bc-ad09-ea124e152507",
    "_uuid": "6b388128bf18f28c29d66377e9deb5f1ea8067f1"
   },
   "source": [
    "## Read data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T18:13:19.922517Z",
     "start_time": "2019-03-01T18:13:18.365574Z"
    },
    "_cell_guid": "7ce644b7-5332-40d7-a827-15f6897be5e8",
    "_uuid": "d1134807fc7b6c604f7fbdd42e0f27e69a834337"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and columns in the train data set: (159571, 8)\n"
     ]
    }
   ],
   "source": [
    "toxic = pd.read_csv('toxicity_data/train.csv') #there's also a test dataset but it doesn't have labels b/c kaggle.\n",
    "print('Number of rows and columns in the train data set:',toxic.shape)\n",
    "\n",
    "#unlabeled data\n",
    "incel_df = pd.read_csv('new_IncelTears_posts.csv')\n",
    "slate_df = pd.read_csv('new_slatestarcodex_posts.csv')\n",
    "\n",
    "#turn multi-class into single class classifier\n",
    "target_col = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']\n",
    "y = toxic[target_col]\n",
    "y['sum'] = y.sum(axis=1).astype(bool).astype(int)\n",
    "\n",
    "#splits data, creates holdout dataset\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(toxic, y, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3a73e542-178e-4b3e-ad8c-87e9dc659762",
    "_uuid": "2daff7626ba426dbfa0172b6c3a4351af27cd4e9"
   },
   "source": [
    "## Text preprocessing - TF-IDF up to trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T18:14:05.412141Z",
     "start_time": "2019-03-01T18:13:20.709192Z"
    },
    "_cell_guid": "93502afb-68c7-4cc2-ad03-f71d2b2cbf2a",
    "_uuid": "3d1747c73d3c67c93eb4e7e81de4400276f0580c"
   },
   "outputs": [],
   "source": [
    "vect_word = TfidfVectorizer(max_features=20000, lowercase=True, analyzer='word',\n",
    "                        stop_words= 'english',ngram_range=(1,3),dtype=np.float32)\n",
    "tr_vect = vect_word.fit_transform(X_train['comment_text'])\n",
    "ts_vect = vect_word.transform(X_test['comment_text'])\n",
    "\n",
    "incel_vect = vect_word.transform(incel_df['title'])\n",
    "slate_vect = vect_word.transform(slate_df['title'])\n",
    "\n",
    "#took 50 seconds on 150k samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fab7e57c-fa64-4540-a53e-085f849d42ca",
    "_uuid": "b15c44a583628a0a72036536e8a5fdb67273a4ae"
   },
   "source": [
    "## LR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T18:14:07.550614Z",
     "start_time": "2019-03-01T18:14:05.414435Z"
    },
    "_cell_guid": "7f5b18c2-6775-4493-ae4b-d7a2456dbdd2",
    "_uuid": "39da8aecea6496cadd1aa2133431cceb440b390c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix\n",
      " [[21668  1211]\n",
      " [  428  2225]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96     22879\n",
      "           1       0.65      0.84      0.73      2653\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     25532\n",
      "   macro avg       0.81      0.89      0.85     25532\n",
      "weighted avg       0.95      0.94      0.94     25532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C=2,random_state = 42,class_weight = 'balanced')\n",
    "lr.fit(tr_vect,y_train['sum'])\n",
    "\n",
    "pred =  lr.predict(ts_vect)\n",
    "print('\\nConfusion matrix\\n',confusion_matrix(y_test['sum'],pred))\n",
    "print(classification_report(y_test['sum'],pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T18:14:07.563187Z",
     "start_time": "2019-03-01T18:14:07.553038Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7308260798160617"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test['sum'],pred)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T23:13:43.595551Z",
     "start_time": "2019-02-27T23:13:41.794438Z"
    }
   },
   "outputs": [],
   "source": [
    "#RUN ONCE TO SAVE MODEL\n",
    "# import pickle\n",
    "# with open('lr_model.pickle', 'wb') as handle:\n",
    "#     pickle.dump(lr, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "### IMPORTS PICKLED LR MODEL\n",
    "# with open('lr_model.pickle', 'rb') as handle:\n",
    "#     lr = pickle.load(handle)\n",
    "\n",
    "#RUN ONCE TO SAVE FIT VECTORIZER\n",
    "# import pickle\n",
    "# with open('fit_vect.pickle', 'wb') as handle:\n",
    "#      pickle.dump(vect_word, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample gridsearch code I could use in further optimizing this model.\n",
    "# http://localhost:8889/notebooks/curriculum/project-04/nlp-overview/NLP%20Overview%20Example/Movies!.ipynb\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# hyper_param_grid = {'C': [0.01, 0.1, 1.0, 10.0]}\n",
    "# lr_tfidf = GridSearchCV(LogisticRegression(), hyper_param_grid, cv=3, n_jobs=-1, verbose=1)\n",
    "# lr.fit(X_train, y_train)\n",
    "# lr.best_estimator_.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at one of our negative & positive subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T00:21:10.279644Z",
     "start_time": "2019-02-27T00:21:10.271336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Incel titles predicted as toxic 0.30792377131394183\n",
      "Percentage of Slate titles predicted as toxic 0.06820461384152457\n",
      "Slate is 4.51x better)\n"
     ]
    }
   ],
   "source": [
    "incel_preds = lr.predict(incel_vect)\n",
    "print(f'Percentage of Incel titles predicted as toxic {incel_preds.sum()/incel_preds.shape[0]}')\n",
    "\n",
    "slate_preds = lr.predict(slate_vect)\n",
    "print(f'Percentage of Slate titles predicted as toxic {slate_preds.sum()/slate_preds.shape[0]}')\n",
    "\n",
    "score_ratio = (incel_preds.sum()/incel_preds.shape[0])/(slate_preds.sum()/slate_preds.shape[0]) #good subreddit 13x better.\n",
    "print (f'Slate is {round(score_ratio,2)}x better)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's eyeball the results too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T00:21:10.293661Z",
     "start_time": "2019-02-27T00:21:10.281294Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Another one thinking he is a genious', '\"But were nonviolent\"',\n",
       "       'Probably a LARP as I imagine the only thing he actually lifts is Cheeto packets. However if true, I hope the next woman be tries it with gives him what he deserves...',\n",
       "       'Because of course he’s entitled to a woman’s body if someone else has had it.',\n",
       "       'They bring so much of their unhappiness upon themselves',\n",
       "       'incel worried about an epidemic of \"open mouthed skinny framed\" guys',\n",
       "       '\"clothes. But her bone structure is terrible, if she was born male, she would be extremely repulsive and for sure involuntary adult virgin\"',\n",
       "       '“Females are the problem, not males.”',\n",
       "       'Incel goes outside and overhears a conversation that had nothing to do with him and takes personal offense.',\n",
       "       'Incel blaming genetics while at the same time refusing to work on himself, you ain’t gonna get anywhere by doing that.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incel_df[np.isin(incel_preds, 0)]['title'].values[:10] #these are the ones it said were ok."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still some pretty bad stuff getting missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T00:21:10.313402Z",
     "start_time": "2019-02-27T00:21:10.295282Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['In Defense of Inclusionism',\n",
       "       'Following up on \"College Has Been Oversold\"',\n",
       "       '*The Elephant in the Brain* Discussion Questions',\n",
       "       'How an Aspiring ‘It’ Girl Tricked New York’s Party People — and Its Banks',\n",
       "       'Public Education’s Dirty Secret',\n",
       "       \"Plummeting insect numbers 'threaten collapse of nature' | Environment | The Guardian\",\n",
       "       'Nature: Human Mind Control of Rat Cyborg’s Continuous Locomotion with Wireless Brain-to-Brain Interface',\n",
       "       'The no-nonsense guide for people who think they might have an eating disorder',\n",
       "       'Why do you think humanity should exist?', 'Hungry Trolls'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slate_df[np.isin(slate_preds, 1)]['title'].values[:10] #these are the ones it said were ok."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still some fine stuff getting misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
